{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOQeHgOl/Wod3Th97k2gsn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chan-woong/self-study/blob/master/Deep-Learning%20part.1%20loss%3D0.23887\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLsKzZ5cnqD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fcdecf3-8f65-471f-d739-94899d0a5403"
      },
      "source": [
        "# one-hot encoding 알아보기\n",
        "# tf.one_hot\n",
        "# 아래 모델을 손봐서 val_loss 0.20 이하로 내리기\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "(x_train, y_train), (x_valid, y_valid) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_valid = x_valid / 255\n",
        "#print(tf.shape(x_train)) # 28x28x3 이미지 x 60000개\n",
        "#print(tf.shape(y_train))\n",
        "x_train = tf.reshape(x_train, (-1 ,28,28,1))\n",
        "x_valid = tf.reshape(x_valid, (-1, 28,28,1))\n",
        "#Conv2D = 합성곱 \n",
        "#MaxPooling2D = 이미지 축소\n",
        "model = Sequential([\n",
        "      Conv2D(128, (3,3),input_shape = (28,28, 1), activation = 'relu'),\n",
        "      MaxPooling2D(2,2),\n",
        "      Conv2D(128, (3,3), activation = 'relu'),\n",
        "      Conv2D(128, (3,3), activation = 'relu'),\n",
        "      Flatten(),\n",
        "      Dense(10, activation = 'softmax'), # sigmoid = Classifier 할때 이진분류에서만 쓰기\n",
        "])\n",
        "model.summary()\n",
        "#softmax일 때 one hot 썼으면 categorical_crossentropy\n",
        "#sigmoid activation 일 경우 binary_crossentropy\n",
        "\n",
        "ckpt_path = \"best.ckpt\"\n",
        "ckpt = ModelCheckpoint(\n",
        "    filepath = ckpt_path,\n",
        "    verbose = 1,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True\n",
        ")\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
        "model.fit(x_train, y_train, validation_data=(x_valid,y_valid), epochs = 20, callbacks = [ckpt])\n",
        "model.load_weights(ckpt_path)\n",
        "\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_95 (Conv2D)           (None, 26, 26, 128)       1280      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_35 (MaxPooling (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_96 (Conv2D)           (None, 11, 11, 128)       147584    \n",
            "_________________________________________________________________\n",
            "conv2d_97 (Conv2D)           (None, 9, 9, 128)         147584    \n",
            "_________________________________________________________________\n",
            "flatten_18 (Flatten)         (None, 10368)             0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 10)                103690    \n",
            "=================================================================\n",
            "Total params: 400,138\n",
            "Trainable params: 400,138\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8566\n",
            "Epoch 00001: val_loss improved from inf to 0.30935, saving model to best.ckpt\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3961 - acc: 0.8566 - val_loss: 0.3093 - val_acc: 0.8949\n",
            "Epoch 2/20\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.2615 - acc: 0.9038\n",
            "Epoch 00002: val_loss improved from 0.30935 to 0.27306, saving model to best.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2618 - acc: 0.9037 - val_loss: 0.2731 - val_acc: 0.9011\n",
            "Epoch 3/20\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.2188 - acc: 0.9197\n",
            "Epoch 00003: val_loss improved from 0.27306 to 0.26403, saving model to best.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2188 - acc: 0.9197 - val_loss: 0.2640 - val_acc: 0.9048\n",
            "Epoch 4/20\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9313\n",
            "Epoch 00004: val_loss improved from 0.26403 to 0.23942, saving model to best.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1867 - acc: 0.9312 - val_loss: 0.2394 - val_acc: 0.9135\n",
            "Epoch 5/20\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.1582 - acc: 0.9419\n",
            "Epoch 00005: val_loss improved from 0.23942 to 0.23887, saving model to best.ckpt\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1583 - acc: 0.9418 - val_loss: 0.2389 - val_acc: 0.9183\n",
            "Epoch 6/20\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9515\n",
            "Epoch 00006: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1315 - acc: 0.9515 - val_loss: 0.2560 - val_acc: 0.9125\n",
            "Epoch 7/20\n",
            "1861/1875 [============================>.] - ETA: 0s - loss: 0.1098 - acc: 0.9602\n",
            "Epoch 00007: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1097 - acc: 0.9603 - val_loss: 0.2761 - val_acc: 0.9151\n",
            "Epoch 8/20\n",
            "1868/1875 [============================>.] - ETA: 0s - loss: 0.0922 - acc: 0.9663\n",
            "Epoch 00008: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0922 - acc: 0.9664 - val_loss: 0.2799 - val_acc: 0.9121\n",
            "Epoch 9/20\n",
            "1870/1875 [============================>.] - ETA: 0s - loss: 0.0777 - acc: 0.9707\n",
            "Epoch 00009: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0777 - acc: 0.9707 - val_loss: 0.3169 - val_acc: 0.9163\n",
            "Epoch 10/20\n",
            "1865/1875 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9751\n",
            "Epoch 00010: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0668 - acc: 0.9750 - val_loss: 0.3237 - val_acc: 0.9155\n",
            "Epoch 11/20\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9777\n",
            "Epoch 00011: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0592 - acc: 0.9778 - val_loss: 0.3798 - val_acc: 0.9155\n",
            "Epoch 12/20\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9811\n",
            "Epoch 00012: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0511 - acc: 0.9811 - val_loss: 0.3805 - val_acc: 0.9156\n",
            "Epoch 13/20\n",
            "1864/1875 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9824\n",
            "Epoch 00013: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0475 - acc: 0.9823 - val_loss: 0.4147 - val_acc: 0.9171\n",
            "Epoch 14/20\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9841\n",
            "Epoch 00014: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0425 - acc: 0.9841 - val_loss: 0.4846 - val_acc: 0.9099\n",
            "Epoch 15/20\n",
            "1867/1875 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9847\n",
            "Epoch 00015: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0413 - acc: 0.9847 - val_loss: 0.4745 - val_acc: 0.9143\n",
            "Epoch 16/20\n",
            "1869/1875 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9862\n",
            "Epoch 00016: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0390 - acc: 0.9862 - val_loss: 0.5066 - val_acc: 0.9132\n",
            "Epoch 17/20\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9871\n",
            "Epoch 00017: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0357 - acc: 0.9871 - val_loss: 0.5550 - val_acc: 0.9123\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - ETA: 0s - loss: 0.0355 - acc: 0.9877\n",
            "Epoch 00018: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0355 - acc: 0.9877 - val_loss: 0.5824 - val_acc: 0.9114\n",
            "Epoch 19/20\n",
            "1863/1875 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9879\n",
            "Epoch 00019: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0337 - acc: 0.9878 - val_loss: 0.5948 - val_acc: 0.9129\n",
            "Epoch 20/20\n",
            "1874/1875 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9882\n",
            "Epoch 00020: val_loss did not improve from 0.23887\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0331 - acc: 0.9882 - val_loss: 0.5629 - val_acc: 0.9154\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1c864404e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}